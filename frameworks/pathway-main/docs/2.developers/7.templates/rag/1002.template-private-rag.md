---
title: "Private RAG App with Mistral and Ollama"
description: "A fully private (local) version of the 'demo-question-answering' RAG pipeline using Pathway, Mistral, and Ollama."
tags: ['showcase', 'ai-pipelines']
date: '2024-04-22'
thumbnail: 
    src: '/assets/content/blog/local-adaptive-rag/local_adaptive.png'
    contain: true
author: "pathway"
layout: "template"
keywords: ['LLM', 'RAG', 'Adaptive RAG', 'prompt engineering', 'explainability', 'mistral', 'ollama', 'private rag', 'local rag', 'ollama rag', 'docker', 'yaml']
docker_github_link: "https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/private-rag"
---

::alert{type="info" icon="heroicons:information-circle-16-solid"}
If you would like to learn more about Private RAG, see our [Private RAG with Connected Data Sources using Mistral, Ollama, and Pathway](/developers/templates/rag/private-rag-ollama-mistral)
::

:ArticleFromUrl{dir="rag" url="private-rag"}
