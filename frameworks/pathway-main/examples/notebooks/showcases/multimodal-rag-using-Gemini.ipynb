{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-colab"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pathwaycom/pathway/blob/main/examples/notebooks/showcases/multimodal-rag-using-Gemini.ipynb\" target=\"_parent\"><img src=\"https://pathway.com/assets/colab-badge.svg\" alt=\"Run In Colab\" class=\"inline\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "notebook-instructions",
      "source": [
        "# Installing Pathway with Python 3.10+\n",
        "\n",
        "In the cell below, we install Pathway into a Python 3.10+ Linux runtime.\n",
        "\n",
        "> **If you are running in Google Colab, please run the colab notebook (Ctrl+F9)**, disregarding the 'not authored by Google' warning.\n",
        "> \n",
        "> **The installation and loading time is less than 1 minute**.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "id": "pip-installation-pathway",
      "source": [
        "%%capture --no-display\n",
        "!pip install --prefer-binary pathway"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "id": "logging",
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.CRITICAL)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "id": "4",
      "metadata": {},
      "source": [
        "::true-img\n",
        "---\n",
        "src: '/assets/content/showcases/gemini_rag/Blog_Banner.png'\n",
        "alt: \"blog banner\"\n",
        "---\n",
        "::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5",
      "metadata": {},
      "source": [
        "# Multimodal RAG with Pathway and Gemini"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6",
      "metadata": {},
      "source": [
        "The recent release of **Google Gemini 1.5**, with its impressive **1 million token context length window**, has sparked discussions about the future of RAG. However, it hasn't rendered it obsolete. This system still offers unique advantages, especially in curating and optimizing the context provided to the model, ensuring relevance and accuracy. What is particularly interesting is how these advancements can be harnessed to enhance our projects and streamline our workflows.\n",
        "\n",
        "In this article, you'll learn how to set up a **Multimodal Retrieval-Augmented Generation (MM-RAG)** system using **Pathway** and **Google Gemini**. You will walk through each step comprehensively, ensuring a solid understanding of both the theoretical and practical aspects of implementing Multimodal LLM and RAG applications.\n",
        "\n",
        "You'll explore how to leverage the capabilities of **Gemini 1.5 Flash** and **Pathway** together. If you're interested in building RAG pipelines with OpenAI, we also have an article on **Multimodal RAG using GPT-4o**, which you can check out [here](/developers/templates/rag/multimodal-rag).\n",
        "\n",
        "If you want to skip the explanations, you can directly find the code [here](#hands-on-multimodal-rag-with-google-gemini).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7",
      "metadata": {},
      "source": [
        "## What this article will cover:\n",
        "- What is Retrieval-Augmented Generation (RAG)?\n",
        "- Multimodality in LLMs\n",
        "- Why is Multimodal RAG (MM-RAG) Needed?\n",
        "- What is Multimodal RAG and Use Cases?\n",
        "- Gemini Models\n",
        "- Release of Gemini 1.5 and its impact on RAG architectures\n",
        "- Comparing LlamaIndex and Pathway\n",
        "- Hands-on Multimodal RAG with Google Gemini"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8",
      "metadata": {},
      "source": [
        "## Foundational Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Why is Multimodal Rag needed?\n",
        "\n",
        "**Retrieval-Augmented Generation (RAG)** enhances large language models by incorporating external knowledge sources before generating responses. This approach ensures relevant and accurate output. In today's data-rich world, documents often combine text and images to convey information comprehensively. However, most Retrieval Augmented Generation (RAG) systems overlook the valuable insights locked within images. As Multimodal Large Language Models (LLMs) gain prominence, it's crucial to explore how we can leverage visual content alongside text in RAG, unlocking a deeper understanding of the information landscape.\n",
        "\n",
        "**Multimodal RAG** is an advanced form of Retrieval-Augmented Generation (RAG) that goes beyond text to incorporate various data types like images, charts, and tables. This expanded capability allows for a deeper understanding of complex information, leading to more accurate and informative outputs.\n",
        "\n",
        "#### Two options for Multimodal RAG\n",
        "1. **Multimodal Embeddings** -\n",
        "The multimodal embeddings model generates vectors based on the input you provide, which can include a combination of image, text, and video data. The image embedding vector and text embedding vector are in the same semantic space with the same dimensionality. Consequently, these vectors can be used interchangeably for use cases like searching image by text, or searching video by image.\n",
        "Utilize multimodal embeddings to integrate text and images, retrieve relevant content through similarity search, and then provide both the raw image and text chunks to a multimodal LLM for answer synthesis.\n",
        "\n",
        "\n",
        "2. **Text Embeddings** -\n",
        "Generate text summaries of images using a multimodal LLM, embed and retrieve the text, and then pass the text chunks to the LLM for answer synthesis.\n",
        "\n",
        "\n",
        "#### Comparing text-based and multimodal RAG\n",
        "Multimodal RAG offers several advantages over text-based RAG:\n",
        "- **Enhanced knowledge access**: Multimodal RAG can access and process both textual and visual information, providing a richer and more comprehensive knowledge base for the LLM.\n",
        "- **Improved reasoning capabilities**: By incorporating visual cues, multimodal RAG can make better informed inferences across different types of data modalities.\n",
        "\n",
        "#### Key Advantages of MM-RAG:\n",
        "- Comprehensive Understanding: Processes multiple data formats for a better picture.\n",
        "- Improved Performance: Visual data enhances efficiency in complex tasks.\n",
        "- Versatile Applications: Useful in finance, healthcare, scientific research, and more.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10",
      "metadata": {},
      "source": [
        "### Gemini Models\n",
        "**Gemini** is Google's most capable and general AI model to date. Google has released several Gemini model variants, each tailored for different use cases and performance requirements.\n",
        "\n",
        "#### Main Gemini Models:\n",
        "- Gemini Ultra: The most powerful and advanced model, capable of handling complex tasks and offering state-of-the-art performance.\n",
        "- Gemini Pro: A versatile model that balances performance and efficiency, suitable for a wide range of applications.\n",
        "- Gemini Advanced: Designed for a broader set of tasks, offering a good balance of capabilities.\n",
        "- Gemini Lite: A smaller, more efficient model focused on speed and responsiveness, ideal for resource-constrained environments.\n",
        "\n",
        "Additional Variants:\n",
        "- Gemini 1.5 Flash: Optimized for high-volume, cost-effective applications.\n",
        "- Gemini 1.5 Pro: Offers a balance of performance and capabilities.\n",
        "- Gemini 1.0 Pro Vision: Includes vision capabilities for processing images and videos.\n",
        "- Gemini 1.0 Pro: Text-based model for general language tasks.\n",
        "\n",
        "#### Benefits of Building with Gemini:\n",
        "**Free Credits**: Google Cloud offers new users up to $300 in free credits. This can be used to experiment with Gemini models and other Google Cloud services.\n",
        "You can also seamlessly integrate MM-RAG applications with Google's Vertex AI platform for streamlined machine learning workflows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "### Release of Gemini 1.5 and its impact on RAG architectures\n",
        "The Gemini 1.5 Flash model, released on May 24, 2024, revolutionized AI with its enhanced speed, efficiency, cost-effectiveness, long context window, and multimodal reasoning capabilities.\n",
        "\n",
        "#### Did Google Gemini 1.5 Kill the need of RAG?\n",
        "In one word **\u201cNo\u201d**. Gemini 1.5, with a 1M context length window, has sparked a new debate about whether RAG (Retrieval Augmented Generation) is still relevant or not. LLMs commonly struggle with hallucination. To address this challenge, two solutions were introduced, one involving an increased context window and the other utilizing RAG. Gemini 1.5 outperforms Claude 2.1 and GPT-4 Turbo as it can assimilate entire code bases, process over 100 papers, and various documents, but it surely hasn\u2019t killed RAG.\n",
        "\n",
        "RAG leverages your private knowledge database for effective Q&A while ensuring the security of sensitive information like trade secrets, confidential IP, GDPR-protected data, and internal documents. For more detailed insights explore our article on Private RAG with Connected Data Sources using Mistral, Ollama, and Pathway [here](/developers/templates/rag/private-rag-ollama-mistral).\n",
        "\n",
        "Additionally in traditional RAG pipelines, you can enhance performance by tweaking the retrieval process, changing the embedding model, adjusting chunking strategies, or improving source data. However, with a \"stuff-the-context-window-1M-tokens\" strategy, your only option is to improve the source data since all data is given to the model within the token limit. Additionally the context window may be filled with many relevant facts, but 40% or more of them are \u201clost\u201d to the model. If you want to make sure the model is actually using the context you are sending it, you are best off curating it first and only sending the most relevant context. In other words, doing traditional RAG.\n",
        "\n",
        "Here in this template you will use the Gemini 1.5 Flash but you can also use other multimodal models by gemini accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12",
      "metadata": {},
      "source": [
        "::true-img\n",
        "---\n",
        "src: '/assets/content/showcases/gemini_rag/gemini1.5flashtable.png'\n",
        "alt: \"Gemini 1.5 flash overview\"\n",
        "---\n",
        "::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13",
      "metadata": {},
      "source": [
        "### Multimodality with Gemini-1.5-Flash\n",
        "Gemini 1.5 Flash is the newest addition to the Gemini family of large language models, and it\u2019s specifically designed to be fast, efficient, and cost-effective for high-volume tasks. This is achieved by being a lighter model than the Gemini 1.5 Pro.\n",
        "\n",
        "According to the paper from Google DeepMind, Gemini 1.5 Flash is \u201ca more lightweight variant designed for efficiency with minimal regression in quality\u201d and uses the transformer decoder model architecture \u201cand multimodal capabilities as Gemini 1.5 Pro, designed for efficient utilization of tensor processing units (TPUs) with lower latency for model serving.\u201d\n",
        "\n",
        "### Gemini 1.5 Flash: Key Features\n",
        "\n",
        "- **Speed and Efficiency**: Fastest Gemini model at 60 tokens/second, ideal for real-time tasks, reducing costs by delaying autoscaling.\n",
        "- **Cost-Effective**: 1/10 the price of Gemini 1.5 Pro and cheaper than GPT-3.5.\n",
        "- **Long Context Window**: Processes up to one million tokens, handling one hour of video, 11 hours of audio, or 700,000 words without losing accuracy.\n",
        "- **Multimodal Reasoning**: Understands text, images, audio, video, PDFs, and tables. Supports function calling and real-time data access.\n",
        "- **Great Performance**: High performance with large context windows, excelling in long-document QA, long-video QA, and long-context ASR.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14",
      "metadata": {},
      "source": [
        "::true-img\n",
        "---\n",
        "src: '/assets/content/showcases/gemini_rag/gemini1.5flashdetails.png'\n",
        "alt: \"Gemini 1.5 flash overview\"\n",
        "---\n",
        "::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15",
      "metadata": {},
      "source": [
        "## Hands on Multimodal RAG with Google Gemini"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16",
      "metadata": {},
      "source": [
        "![Gemini RAG overview](https://pathway.com/assets/content/showcases/gemini_rag/RAG_diagram.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17",
      "metadata": {},
      "source": [
        "### Step 1: Installation\n",
        "\n",
        "First, we need to install the required packages: pathway[all], litellm==1.40.0 and google-generativeai."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install 'pathway[all]>=0.14.0' litellm==1.40.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19",
      "metadata": {},
      "source": [
        "### Step 2: Imports and Environment Setup\n",
        "\n",
        "Next, we import the necessary libraries and set up the environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20",
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21",
      "metadata": {},
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22",
      "metadata": {},
      "outputs": [],
      "source": [
        "import litellm\n",
        "\n",
        "import pathway as pw\n",
        "\n",
        "# To use advanced features with Pathway Scale, get your free license key from\n",
        "# https://pathway.com/features and paste it below.\n",
        "# To use Pathway Community, comment out the line below.\n",
        "pw.set_license_key(\"demo-license-key-with-telemetry\")\n",
        "\n",
        "from pathway.udfs import DiskCache, ExponentialBackoffRetryStrategy\n",
        "from pathway.xpacks.llm import embedders, llms, parsers, prompts, splitters\n",
        "from pathway.xpacks.llm.question_answering import BaseRAGQuestionAnswerer\n",
        "from pathway.xpacks.llm.vector_store import VectorStoreServer\n",
        "\n",
        "# Set the logging level for LiteLLM to DEBUG\n",
        "os.environ[\"LITELLM_LOG\"] = \"DEBUG\"  # to help in debugging"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23",
      "metadata": {},
      "source": [
        "### Step 3: API Key Setup and License Key Setup\n",
        "\n",
        "Set up the API key and the Pathway license key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Api key setup\n",
        "GEMINI_API_KEY = \"Paste your Gemini API Key here\"\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
        "os.environ[\"TESSDATA_PREFIX\"] = \"/usr/share/tesseract/tessdata/\"\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# License key setup\n",
        "pw.set_license_key(\"demo-license-key-with-telemetry\")\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25",
      "metadata": {},
      "source": [
        "### Step 4: Upload your file\n",
        "\n",
        "Create a `./data` directory if it doesn't already exist. This is where the uploaded files will be stored. Then upload your pdf documents.\n",
        "\n",
        "You can also omit this cell if you are running locally on your system - in that case create a `data` folder in the current directory and copy the files and comment out this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "!mkdir -p data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# Demo pdf for testing\n",
        "!wget -q -P ./data/ https://github.com/pathwaycom/llm-app/raw/main/examples/pipelines/gpt_4o_multimodal_rag/data/20230203_alphabet_10K.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28",
      "metadata": {},
      "source": [
        "#### Reading PDF Data\n",
        "\n",
        "Next, we read the PDF data from a folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the PDF data\n",
        "folder = pw.io.fs.read(\n",
        "    path=\"./data/\",\n",
        "    format=\"binary\",\n",
        "    with_metadata=True,\n",
        ")\n",
        "sources = [folder]  # you can add any other Pathway connector here!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30",
      "metadata": {},
      "source": [
        "### Step 5: Document Processing and Question Answering Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31",
      "metadata": {},
      "source": [
        "#### Setting Up LiteLLM Chat\n",
        "\n",
        "Set up a LiteLLM chat instance with retry and cache strategies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup LiteLLM chat\n",
        "chat = llms.LiteLLMChat(\n",
        "    model=\"gemini/gemini-1.5-flash\",  # Model specified for LiteLLM\n",
        "    retry_strategy=ExponentialBackoffRetryStrategy(max_retries=6, backoff_factor=2.5),\n",
        "    temperature=0.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33",
      "metadata": {},
      "source": [
        "#### Setting Up Embedder\n",
        "\n",
        "Let's utilize Gemini embedders. The `GeminiEmbedder` class in Pathway provides an interface for interacting with Gemini embedders. It generates semantic embeddings with a specified model, providing methods for single items (`embed`), batches (`embed_batch`), and direct calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup embedder\n",
        "embedder = embedders.GeminiEmbedder(\n",
        "    model=\"models/embedding-001\",\n",
        "    retry_strategy=ExponentialBackoffRetryStrategy(max_retries=6, backoff_factor=2.5),\n",
        ")  # Specify embedder here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35",
      "metadata": {},
      "source": [
        "#### Setting Up Parser\n",
        "\n",
        "Next, we set up a parser for the document store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup parser\n",
        "table_args = {\n",
        "    \"parsing_algorithm\": \"llm\",  # for tables\n",
        "    \"llm\": chat,\n",
        "    \"prompt\": prompts.DEFAULT_MD_TABLE_PARSE_PROMPT,\n",
        "}\n",
        "\n",
        "image_args = {\n",
        "    \"parsing_algorithm\": \"llm\",  # for images\n",
        "    \"llm\": chat,\n",
        "    \"prompt\": prompts.DEFAULT_IMAGE_PARSE_PROMPT,\n",
        "}\n",
        "\n",
        "parser = parsers.DoclingParser(multimodal_llm=chat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37",
      "metadata": {},
      "source": [
        "#### Setting Up Document Store\n",
        "\n",
        "We will set up the document store with the sources, embedder, and parser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup document store\n",
        "# splitter = splitters.TokenCountSplitter()\n",
        "doc_store = VectorStoreServer(\n",
        "    *sources,\n",
        "    embedder=embedder,\n",
        "    splitter=splitter,\n",
        "    parser=parser,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39",
      "metadata": {},
      "source": [
        "### Step 6: Setting Up Question Answerer Application\n",
        "\n",
        "We will set up the question answerer application using the LiteLLM-based chat object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup question answerer application\n",
        "app = BaseRAGQuestionAnswerer(\n",
        "        llm=chat,  # Using the LiteLLM-based chat object\n",
        "        indexer=doc_store, search_topk=2,\n",
        "        short_prompt_template=prompts.prompt_qa)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41",
      "metadata": {},
      "source": [
        "#### Building and Running the Server\n",
        "\n",
        "Finally, we build and run the server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build and run the server\n",
        "app_host = \"0.0.0.0\"\n",
        "app_port = 8000\n",
        "app.build_server(host=app_host, port=app_port)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43",
      "metadata": {},
      "outputs": [],
      "source": [
        "import threading\n",
        "t = threading.Thread(target=app.run_server, name=\"BaseRAGQuestionAnswerer\")\n",
        "t.daemon = True\n",
        "thr = t.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathway.xpacks.llm.question_answering import RAGClient\n",
        "\n",
        "# Initialize the RAG client\n",
        "client = RAGClient(host=\"0.0.0.0\", port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "$256,144 million\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "\n",
        "response = client.answer(\"What is the Total Stockholders' equity as of December 31, 2022?\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46",
      "metadata": {},
      "source": [
        "Now your chatbot is now running live! You can ask any questions and get information from your documents instantly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This article demonstrated how to implement a Multimodal RAG service using Pathway and Gemini. The setup leverages the capabilities of LiteLLM to process and query multimodal data effectively. If you're looking for a cost-effective alternative, consider using the Gemini Mini, which provides great performance at a lower cost.\n",
        "\n",
        "For more detailed insights and an alternative approach, check out our article on multimodal RAG using GPT-4o [here](/developers/templates/rag/multimodal-rag). This will give you another perspective on how to handle multimodal RAG applications using different models and techniques.\n",
        "By following the steps outlined above, you can efficiently integrate and utilize various data types to enhance your AI applications, ensuring more accurate and contextually rich outputs.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}